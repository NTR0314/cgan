# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gpPKNnOV6aro3coD5_BNUgjcKi89Urpe
"""

# Download and extract data
!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
!tar -xzf cifar-10-python.tar.gz
!ls

import argparse
import os
import random
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
import numpy as np
import pickle
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML
import time
import torch.nn.functional as F

# Set random seed for reproducibility
manualSeed = 1337
# manualSeed = random.randint(1, 10000) # use if you want new results
print("Random Seed: ", manualSeed)
random.seed(manualSeed)
torch.manual_seed(manualSeed)

# Root directory for dataset
dataroot = "data/cifar10"

# Number of workers for dataloader
workers = 2

# Batch size during training
batch_size = 128

# Spatial size of training images. All images will be resized to this
#   size using a transformer.

# Number of channels in the training images. For color images this is 3
nc = 3

# Size of z latent vector (i.e. size of generator input)
nz = 100

# Size of feature maps in generator
ngf = 64

# Size of feature maps in discriminator
ndf = 64

# Learning rate for optimizers
lr = 0.0002

# Beta1 hyperparam for Adam optimizers
beta1 = 0.5

# Number of GPUs available. Use 0 for CPU mode.
ngpu = 1

# Import CIFAR data

def unpickle(file):
    with open(file, 'rb') as fo:
        out = pickle.load(fo, encoding='bytes')
    return out

def pickle_save(file, obj):
    with open(file, 'wb') as f:
        pickle.dump(obj,f)
    return

data_batches_data = torch.empty(0)
data_batches_label = torch.empty(0)

for i in range(1, 6):
    file_path = "cifar-10-batches-py/data_batch_" + str(i)
    data_batch = unpickle(file_path)
    data_batch_feats = torch.tensor(data_batch[b'data'])
    data_batch_len = data_batch_feats.shape[0]
    data_batch_feats = (torch.reshape(data_batch_feats, (data_batch_len, 3, 32, 32)) - 127.5) / 127.5
    
    data_batches_data = torch.cat((data_batches_data, data_batch_feats), 0)
    data_batches_label = torch.cat((data_batches_label, torch.tensor(data_batch[b'labels'])), 0)
    
# squeeze batchtes
data_batches_data = torch.squeeze(data_batches_data)
data_batches_label = torch.squeeze(data_batches_label)

label_names = unpickle("cifar-10-batches-py/batches.meta")[b'label_names']

test_batch = unpickle("cifar-10-batches-py/test_batch")
test_batch_data = torch.tensor(test_batch[b'data'])
test_batch_len = test_batch_data.shape[0]
test_batch_data = (torch.reshape(test_batch_data, (test_batch_len, 3, 32, 32)) - 127.5) / 127.5
test_batch_label = torch.tensor(test_batch[b'labels'])

# Create Dataset

from torch.utils.data import Dataset
import numpy as np

class CIFARDataset(Dataset):
  def __init__(self, test=False): # Constructor
        if test:
            self.data = list(zip(test_batch_data, test_batch_label))
        else:
            self.data = list(zip(data_batches_data, data_batches_label))

  def __len__(self):
      return len(self.data)

  def __getitem__(self, index: int):
    feat, label = self.data[index]
    
    return {'label': label, 'feat': feat}

dataset_train = CIFARDataset()
dataset_test = CIFARDataset(test=True)

# Check dataloader
dataloader = torch.utils.data.DataLoader(dataset_train, 
                                        batch_size=batch_size, 
                                        shuffle=True, 
                                        num_workers=workers)

# Decide which device we want to run on
device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")

# Plot some training images
real_batch = next(iter(dataloader))
plt.figure(figsize=(8,8))
plt.axis("off")
plt.title("Training Images")

plt.imshow(np.transpose(vutils.make_grid(real_batch['feat'].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))

class Generator(nn.Module):
    def __init__(self, ngpu, nz, ngf, nc):
        super(Generator, self).__init__()
        # Don't know what ngpu is used for
        self.ngpu = ngpu
        self.label_upscale = nn.Sequential(
            nn.ConvTranspose2d(10, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8), # Maybe this can also be reported as an improvement
            nn.ReLU(True)
        )
        self.img_upscale = nn.Sequential(
            # input is Z, going into a convolution
            # This is supposed to be used as an improvement over Convolution + Upsamling
            #  convTranspose2d args: c_in, c_out, kernel_size, stride, padding
            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8), # Maybe this can also be reported as an improvement
            nn.ReLU(True)
        )
        
        self.main = nn.Sequential(
            # state size. (ngf*8) x 4 x 4
            nn.ConvTranspose2d(ngf * 8 * 2, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            # state size. (ngf*4) x 8 x 8
            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            # state size. (ngf*2) x 16 x 16
            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size. (nc) x 32 x 32
        )

    def forward(self, input_image, input_label):
        one_hot_label = F.one_hot(input_label.long(), num_classes=10).float()
        # b x 10 -> b x 10 x 1 x 1
        one_hot_label = one_hot_label.unsqueeze(-1).unsqueeze(-1)
        
        lbl_4 = self.label_upscale(one_hot_label)
        inp_img_4 = self.img_upscale(input_image)
        # Cat | dim: N x C x 4 x 4
        catvec = torch.cat((inp_img_4, lbl_4), dim=1)
        
        return self.main(catvec)

# Decide which device we want to run on
device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")

# custom weights initialization called on netG and netD
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

# Create the generator
nz = 100 + 10 # 100 normal dz + 10 dims of one-hot encoded label
ngf = 128
nc = 3

netG = Generator(ngpu, nz, ngf, nc).to(device)

# Handle multi-gpu if desired
if (device.type == 'cuda') and (ngpu > 1):
    netG = nn.DataParallel(netG, list(range(ngpu)))

# Apply the weights_init function to randomly initialize all weights
#  to mean=0, stdev=0.02.
netG.apply(weights_init)

# Print the model
print(netG)

class Discriminator(nn.Module):
    def __init__(self, ngpu, nz, ngf, nc):
        super(Discriminator, self).__init__()
        self.ngpu = ngpu
        # 10 x 1 x 1 -> 1 x 16 x 16
        self.label_conv = nn.ConvTranspose2d(10, 1, 16, 1)
        # nc x 32 x 32 -> 2ndf x 16 x 16
        self.ds_img = nn.Sequential(
            nn.Conv2d(nc, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True)
        )
        
        self.main = nn.Sequential(
            # state size. (ndf) x 16 x 16
            nn.Conv2d(ndf * 2 + 1, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*2) x 8 x 8
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*8) x 4 x 4
            # conv2d args: c_in, c_out, kernel_size, stride, padding
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, input_image, label: torch.Tensor):
        # One hot of labels \in [0, 9]
        one_hot_label = F.one_hot(label.long(), num_classes=10).float()
        # reshape b x 10 -> b x 10 x 1 x 1
        one_hot_label = one_hot_label.unsqueeze(-1).unsqueeze(-1)
        # Scale labels to same as image after first conv layer like: https://www.researchgate.net/publication/331915834_Designing_nanophotonic_structures_using_conditional-deep_convolutional_generative_adversarial_networks
        label_upscaled = self.label_conv(one_hot_label)
        # Concatenate upscaled labels and downscaled img.
        downscaled_image = self.ds_img(input_image)
        # Cat, dim: batch x C x 16 x 16
        concated = torch.cat((downscaled_image, label_upscaled), dim=1)
        
        return self.main(concated)

# Create the Discriminator
netD = Discriminator(ngpu, nz, ngf, nc).to(device)

# Handle multi-gpu if desired
if (device.type == 'cuda') and (ngpu > 1):
    netD = nn.DataParallel(netD, list(range(ngpu)))

# Apply the weights_init function to randomly initialize all weights
#  to mean=0, stdev=0.2.
netD.apply(weights_init)

# Print the model
print(netD)

# Initialize BCELoss function
criterion = nn.BCELoss()

# Create batch of latent vectors that we will use to visualize
#  the progression of the generator
# randn args: input dims -> 64 x nz (=100) x 1 x 1
fixed_noise = torch.randn(64, nz, 1, 1, device=device)
# shape: 64
fixed_labels = torch.arange(8).unsqueeze(-1).repeat(1, 8).view(-1).to(device)


# Establish convention for real and fake labels during training
real_label = 1.
fake_label = 0.

# Setup Adam optimizers for both G and D
optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))

# Load model
# netD.load_state_dict(torch.load('model_weights_netD.pth'))
# netG.load_state_dict(torch.load('model_weights_netG.pth'))

# Commented out IPython magic to ensure Python compatibility.
# Training Loop

# Lists to keep track of progress
img_list = []
G_losses = []
D_losses = []
iters = 0
num_epochs = 100

print("Starting Training Loop...")
# For each epoch
for epoch in range(num_epochs):
    # For each batch in the dataloader
    for i, data in enumerate(dataloader, 0):

        ############################
        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
        ###########################
        ## Train with all-real batch
        netD.zero_grad()
        # Format batch
        real_cpu = data['feat'].to(device)
        real_img_labels = data['label'].to(device)
        b_size = real_cpu.size(0)
        # torch.full args: size, fill_value -> batch_size großeen tensor mit 1 = echtes Bild erstellen
        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)
        # Forward pass real batch through D
        # output of netD is 0/1 depending on whether it guesses it is fake or not
        # view(-1) sorgt dafür dass alle in einer grossen liste sind um mit label zu vergleichen
        output = netD(real_cpu, real_img_labels).view(-1)
        # Calculate loss on all-real batch
        errD_real = criterion(output, label)
        # Calculate gradients for D in backward pass
        errD_real.backward()
        D_x = output.mean().item()

        ## Train with all-fake batch
        # Generate batch of latent vectors
        noise = torch.randn(b_size, nz, 1, 1, device=device)
        # Generate fake image batch with G
        fake = netG(noise, real_img_labels)
        label.fill_(fake_label)
        # Classify all fake batch with D
        output = netD(fake.detach(), real_img_labels).view(-1)
        # Calculate D's loss on the all-fake batch
        errD_fake = criterion(output, label)
        # Calculate the gradients for this batch, accumulated (summed) with previous gradients
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        # Compute error of D as sum over the fake and the real batches
        errD = errD_real + errD_fake
        # Update D
        optimizerD.step()

        ############################
        # (2) Update G network: maximize log(D(G(z)))
        ###########################
        netG.zero_grad()
        label.fill_(real_label)  # fake labels are real for generator cost
        # Since we just updated D, perform another forward pass of all-fake batch through D
        output = netD(fake, real_img_labels).view(-1)
        # Calculate G's loss based on this output
        errG = criterion(output, label)
        # Calculate gradients for G
        errG.backward()
        D_G_z2 = output.mean().item()
        # Update G
        optimizerG.step()

        # Output training stats
        if i % 50 == 0:
            print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'
#                   % (epoch, num_epochs, i, len(dataloader),
                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))

        # Save Losses for plotting later
        G_losses.append(errG.item())
        D_losses.append(errD.item())

        iters += 1
    # Check how the generator is doing by saving G's output on fixed_noise
    with torch.no_grad():
        fake = netG(fixed_noise, fixed_labels).detach().cpu()
        img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

with torch.no_grad():
    fake = netG(fixed_noise, fixed_labels).detach().cpu()
    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

# Save img_list for visualization
timestr = time.strftime("%Y_%m_%d-%H_%M")
pickle_save(f'img_list_{timestr}_{num_epochs}.pckl' ,img_list)

# Save model after training:
# https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html#save-and-load-the-model
torch.save(netD.state_dict(), f'model_weights_netD_{timestr}_{num_epochs}.pth')
torch.save(netG.state_dict(), f'model_weights_netG_{timestr}_{num_epochs}.pth')

fig = plt.figure(figsize=(8,8))
plt.axis("off")
ims = [[plt.imshow(np.transpose(i,(1, 2,0)), animated=True)] for i in img_list[::3]]
ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)

HTML(ani.to_jshtml())

plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(G_losses,label="G")
plt.plot(D_losses,label="D")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Grab a batch of real images from the dataloader
real_batch = next(iter(dataloader))

# Plot the real images
plt.figure(figsize=(15,15))
plt.subplot(1,2,1)
plt.axis("off")
plt.title("Real Images")
plt.imshow(np.transpose(vutils.make_grid(real_batch['feat'].to(device)[:64], padding=2, normalize=True).cpu(), (1, 2,0)))

# Plot the fake images from the last epoch
plt.subplot(1,2,2)
plt.axis("off")
plt.title("Fake Images")
plt.imshow(np.transpose(img_list[-1], (1,2,0)))
plt.show()

import torch
from torch import nn
from torchvision.models import inception_v3  
from scipy import linalg 

class PartialInceptionNetwork(nn.Module):

    def __init__(self, transform_input=True):
        super().__init__()
        self.inception_network = inception_v3(weights='Inception_V3_Weights.DEFAULT') 
        self.inception_network.Mixed_7c.register_forward_hook(self.output_hook)
        self.transform_input = transform_input

    def output_hook(self, module, input, output):
        # N x 2048 x 8 x 8
        self.mixed_7c_output = output

    def forward(self, x):
        """
        Args:
            x: shape (N, 3, 299, 299) dtype: torch.float32 in range 0-1
        Returns:
            inception activations: torch.tensor, shape: (N, 2048), dtype: torch.float32
        """
        assert x.shape[1:] == (3, 299, 299), "Expected input shape to be: (N,3,299,299)" +\
                                             ", but got {}".format(x.shape)
        x = x * 2 -1 # Normalize to [-1, 1]

        # Trigger output hook
        self.inception_network(x)

        # Output: N x 2048 x 1 x 1 
        activations = self.mixed_7c_output
        activations = torch.nn.functional.adaptive_avg_pool2d(activations, (1,1))
        activations = activations.view(x.shape[0], 2048)
        return activations

from PIL import Image
import torchvision.transforms as T
import multiprocessing

def resize_image(tensor, transform1, transform2):
    image = transform1(tensor)
    new_image = image.resize((299, 299), Image.Resampling.LANCZOS)
    new_tensor = transform2(new_image) 
    return new_tensor

def resize_images(tensors):
    transform1 = T.ToPILImage()
    transform2 = T.ToTensor() 
    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:
        jobs = []
        for tensor in tensors:
            job = pool.apply_async(resize_image, (tensor, transform1, transform2))
            jobs.append(job)
        new_tensors = torch.zeros(tensors.shape[0], 3, 299, 299)
        for idx, job in enumerate(jobs):
            new_tensor = job.get()
            new_tensors[idx] = new_tensor 
    new_tensors = torch.squeeze(new_tensors)
    return new_tensors

def get_activations(images):
    """
    Calculates activations for last pool layer for all iamges
    --
        Images: torch.array shape: (N, 3, 299, 299), dtype: torch.float32
        batch size: batch size used for inception network
    --
    Returns: np array shape: (N, 2048), dtype: np.float32
    """
    assert images.shape[1:] == (3, 299, 299), "Expected input shape to be: (N,3,299,299)" +\
                                              ", but got {}".format(images.shape)
    
    torch.cuda.empty_cache() 
    num_images = images.shape[0]
    with torch.no_grad(): 
        inception_network = PartialInceptionNetwork()
        if torch.cuda.is_available():
            inception_network = inception_network.cuda() 
        inception_network.eval()
        n_batches = int(np.ceil(num_images  / batch_size))
        inception_activations = np.zeros((num_images, 2048), dtype=np.float32)
        for batch_idx in range(n_batches): 
            start_idx = batch_size * batch_idx
            end_idx = batch_size * (batch_idx + 1)

            ims = images[start_idx:end_idx] 
            if torch.cuda.is_available():
                ims = ims.cuda() 
            activations = inception_network(ims)
            activations = activations.detach().cpu().numpy()
            assert activations.shape == (ims.shape[0], 2048), "Expexted output shape to be: {}, but was: {}".format((ims.shape[0], 2048), activations.shape)
            inception_activations[start_idx:end_idx, :] = activations
    return inception_activations

def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):
    """Numpy implementation of the Frechet Distance.
    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)
    and X_2 ~ N(mu_2, C_2) is
            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).
            
    Stable version by Dougal J. Sutherland.
    Params:
    -- mu1 : Numpy array containing the activations of the pool_3 layer of the
             inception net ( like returned by the function 'get_predictions')
             for generated samples.
    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted
               on an representive data set.
    -- sigma1: The covariance matrix over activations of the pool_3 layer for
               generated samples.
    -- sigma2: The covariance matrix over activations of the pool_3 layer,
               precalcualted on an representive data set.
    Returns:
    --   : The Frechet Distance.
    """

    mu1 = np.atleast_1d(mu1)
    mu2 = np.atleast_1d(mu2)

    sigma1 = np.atleast_2d(sigma1)
    sigma2 = np.atleast_2d(sigma2)

    assert mu1.shape == mu2.shape, "Training and test mean vectors have different lengths"
    assert sigma1.shape == sigma2.shape, "Training and test covariances have different dimensions"

    diff = mu1 - mu2
    # product might be almost singular
    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
    if not np.isfinite(covmean).all():
        msg = "fid calculation produces singular product; adding %s to diagonal of cov estimates" % eps
        warnings.warn(msg)
        offset = np.eye(sigma1.shape[0]) * eps
        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))

    # numerical error might give slight imaginary component
    if np.iscomplexobj(covmean):
        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):
            m = np.max(np.abs(covmean.imag))
            raise ValueError("Imaginary component {}".format(m))
        covmean = covmean.real

    tr_covmean = np.trace(covmean)

    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean


def calculate_fid(images1, images2):
    """ Calculate FID between images1 and images2
    Args:
        images1: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8
        images2: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8
        use_multiprocessing: If multiprocessing should be used to pre-process the images
        batch size: batch size used for inception network
    Returns:
        FID (scalar)
    """
    act1 = get_activations(images1)
    mu1 = np.mean(act1, axis=0)
    sigma1 = np.cov(act1, rowvar=False)
    
    act2 = get_activations(images2)
    mu2 = np.mean(act2, axis=0)
    sigma2 = np.cov(act2, rowvar=False)
    
    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)
    return fid

images1 = resize_images(real_batch['feat']) 
images2 = resize_images(fake)
fid_value = calculate_fid(images1, images2)
print('FID: %.3f' % fid_value)